{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# QR Code Phishing Detection using Deep Learning\n",
        "\n",
        "**Project:** End-of-Workshop Assignment - Deep Learning with PyTorch  \n",
        "**Dataset:** QR Code Images (Benign vs Malicious)  \n",
        "**Model:** Convolutional Neural Network (CNN)  \n",
        "**Task:** Binary Classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Problem Statement\n",
        "\n",
        "### Objective\n",
        "Detect phishing attempts by classifying QR codes as either **benign** or **malicious** using deep learning.\n",
        "\n",
        "### Motivation\n",
        "- QR codes are increasingly used in phishing attacks (\"quishing\")\n",
        "- Need automated detection system\n",
        "- Visual patterns in QR codes may indicate malicious intent\n",
        "\n",
        "### Dataset\n",
        "- **Benign QR codes**: ~430,000 images\n",
        "- **Malicious QR codes**: ~576,000 images\n",
        "- **Total**: ~1,006,000 QR code images\n",
        "\n",
        "### Task\n",
        "Binary classification: Predict if a QR code is benign (0) or malicious (1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import yaml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Import project modules\n",
        "from dataset import QRCodeDataset, get_transforms\n",
        "from data_utils import create_data_splits, create_dataloaders\n",
        "from model import create_model, create_optimizer, create_scheduler\n",
        "from train import train\n",
        "from test import evaluate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Analysis (EDA)\n",
        "\n",
        "### 3.1 Dataset Overview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load configuration\n",
        "with open('config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(\"Dataset Configuration:\")\n",
        "print(f\"  - Benign directory: {config['data']['benign_dir']}\")\n",
        "print(f\"  - Malicious directory: {config['data']['malicious_dir']}\")\n",
        "print(f\"  - Sample size per class: {config['data']['sample_size']}\")\n",
        "print(f\"  - Image size: {config['data']['image_size']}x{config['data']['image_size']}\")\n",
        "print(f\"  - Train/Val/Test split: {config['data']['train_ratio']:.0%}/{config['data']['val_ratio']:.0%}/{config['data']['test_ratio']:.0%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Check Available Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "benign_dir = Path(config['data']['benign_dir'])\n",
        "malicious_dir = Path(config['data']['malicious_dir'])\n",
        "\n",
        "# Count available images\n",
        "def count_images(directory):\n",
        "    extensions = {'.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'}\n",
        "    count = sum(1 for f in directory.iterdir() if f.suffix in extensions)\n",
        "    return count\n",
        "\n",
        "benign_count = count_images(benign_dir)\n",
        "malicious_count = count_images(malicious_dir)\n",
        "\n",
        "print(f\"Available Images:\")\n",
        "print(f\"  - Benign: {benign_count:,} images\")\n",
        "print(f\"  - Malicious: {malicious_count:,} images\")\n",
        "print(f\"  - Total: {benign_count + malicious_count:,} images\")\n",
        "print(f\"\\nUsing: {config['data']['sample_size']:,} per class = {config['data']['sample_size'] * 2:,} total\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Visualize Sample Images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a few sample images for visualization\n",
        "sample_size = 100  # Small sample for EDA\n",
        "train_dataset_eda, _, _ = create_data_splits(\n",
        "    benign_dir=str(benign_dir),\n",
        "    malicious_dir=str(malicious_dir),\n",
        "    sample_size=sample_size,\n",
        "    train_ratio=0.7,\n",
        "    val_ratio=0.15,\n",
        "    test_ratio=0.15,\n",
        "    image_size=config['data']['image_size'],\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# Visualize sample images\n",
        "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(8):\n",
        "    image, label = train_dataset_eda[i]\n",
        "    \n",
        "    # Denormalize for visualization\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "    img_vis = image * std + mean\n",
        "    img_vis = torch.clamp(img_vis, 0, 1)\n",
        "    \n",
        "    axes[i].imshow(img_vis.permute(1, 2, 0).numpy())\n",
        "    axes[i].set_title(f\"{'Benign' if label == 0 else 'Malicious'}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.suptitle('Sample QR Code Images', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Class Distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create full dataset for analysis\n",
        "train_dataset_full, val_dataset_full, test_dataset_full = create_data_splits(\n",
        "    benign_dir=str(benign_dir),\n",
        "    malicious_dir=str(malicious_dir),\n",
        "    sample_size=config['data']['sample_size'],\n",
        "    train_ratio=config['data']['train_ratio'],\n",
        "    val_ratio=config['data']['val_ratio'],\n",
        "    test_ratio=config['data']['test_ratio'],\n",
        "    image_size=config['data']['image_size'],\n",
        "    seed=config['data']['seed']\n",
        ")\n",
        "\n",
        "# Get class distribution\n",
        "train_dist = train_dataset_full.get_class_distribution()\n",
        "val_dist = val_dataset_full.get_class_distribution()\n",
        "test_dist = test_dataset_full.get_class_distribution()\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for idx, (name, dist) in enumerate([('Train', train_dist), ('Validation', val_dist), ('Test', test_dist)]):\n",
        "    labels = ['Benign', 'Malicious']\n",
        "    sizes = [dist['benign'], dist['malicious']]\n",
        "    colors = ['#66b3ff', '#ff9999']\n",
        "    \n",
        "    axes[idx].pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)\n",
        "    axes[idx].set_title(f'{name} Set\\n({dist[\"total\"]} images)')\n",
        "\n",
        "plt.suptitle('Class Distribution Across Splits', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nClass Distribution Summary:\")\n",
        "print(f\"Train: {train_dist['benign']} benign, {train_dist['malicious']} malicious\")\n",
        "print(f\"Val:   {val_dist['benign']} benign, {val_dist['malicious']} malicious\")\n",
        "print(f\"Test:  {test_dist['benign']} benign, {test_dist['malicious']} malicious\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = create_model(\n",
        "    num_classes=config['model']['num_classes'],\n",
        "    dropout=config['model']['dropout'],\n",
        "    device=device\n",
        ")\n",
        "\n",
        "print(\"Model Architecture:\")\n",
        "print(model)\n",
        "print(f\"\\nModel Statistics:\")\n",
        "print(f\"  - Total parameters: {model.count_parameters():,}\")\n",
        "print(f\"  - Model size: {model.get_model_size_mb():.2f} MB\")\n",
        "\n",
        "# Test forward pass\n",
        "dummy_input = torch.randn(1, 3, config['data']['image_size'], config['data']['image_size']).to(device)\n",
        "with torch.no_grad():\n",
        "    output = model(dummy_input)\n",
        "print(f\"  - Input shape: {dummy_input.shape}\")\n",
        "print(f\"  - Output shape: {output.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Starting Training...\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  - Epochs: {config['training']['epochs']}\")\n",
        "print(f\"  - Batch size: {config['training']['batch_size']}\")\n",
        "print(f\"  - Learning rate: {config['training']['learning_rate']}\")\n",
        "print(f\"  - Sample size: {config['data']['sample_size']} per class\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Train model\n",
        "trained_model, history = train(\n",
        "    config_path='config.yaml',\n",
        "    save_plots=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Training History Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "# Loss plot\n",
        "axes[0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', linewidth=2)\n",
        "axes[0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', linewidth=2)\n",
        "axes[0].set_xlabel('Epoch', fontsize=12)\n",
        "axes[0].set_ylabel('Loss', fontsize=12)\n",
        "axes[0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=11)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy plot\n",
        "axes[1].plot(epochs, history['train_acc'], 'b-', label='Train Acc', linewidth=2)\n",
        "axes[1].plot(epochs, history['val_acc'], 'r-', label='Val Acc', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch', fontsize=12)\n",
        "axes[1].set_ylabel('Accuracy (%)', fontsize=12)\n",
        "axes[1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=11)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print best metrics\n",
        "best_val_epoch = np.argmin(history['val_loss']) + 1\n",
        "print(f\"\\nBest Validation Performance:\")\n",
        "print(f\"  - Epoch: {best_val_epoch}\")\n",
        "print(f\"  - Val Loss: {min(history['val_loss']):.4f}\")\n",
        "print(f\"  - Val Accuracy: {max(history['val_acc']):.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate on test set\n",
        "results = evaluate(\n",
        "    config_path='config.yaml',\n",
        "    model_path=None,  # Uses best model from training\n",
        "    save_plots=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Confusion Matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrix\n",
        "cm = results['confusion_matrix']\n",
        "class_names = ['Benign', 'Malicious']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=class_names,\n",
        "    yticklabels=class_names,\n",
        "    cbar_kws={'label': 'Count'}\n",
        ")\n",
        "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print metrics\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"  - Accuracy: {results['accuracy']*100:.2f}%\")\n",
        "print(f\"  - Precision: {results['precision']*100:.2f}%\")\n",
        "print(f\"  - Recall: {results['recall']*100:.2f}%\")\n",
        "print(f\"  - F1-Score: {results['f1']*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nPer-Class Metrics:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    print(f\"  {class_name}:\")\n",
        "    print(f\"    Precision: {results['precision_per_class'][i]:.4f}\")\n",
        "    print(f\"    Recall: {results['recall_per_class'][i]:.4f}\")\n",
        "    print(f\"    F1-Score: {results['f1_per_class'][i]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Efficiency Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if 'inference_time' in results:\n",
        "    timing = results['inference_time']\n",
        "    \n",
        "    print(f\"\\nEfficiency Metrics:\")\n",
        "    print(f\"  - Average inference time: {timing['avg_sample_time_ms']:.2f} ms/sample\")\n",
        "    print(f\"  - Throughput: {timing['samples_per_second']:.2f} samples/sec\")\n",
        "    print(f\"  - Model parameters: {trained_model.count_parameters():,}\")\n",
        "    print(f\"  - Model size: {trained_model.get_model_size_mb():.2f} MB\")\n",
        "    \n",
        "    # Visualize inference time\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    metrics = ['Inference Time\\n(ms/sample)', 'Throughput\\n(samples/sec)']\n",
        "    values = [timing['avg_sample_time_ms'], timing['samples_per_second']]\n",
        "    \n",
        "    bars = ax.bar(metrics, values, color=['#66b3ff', '#99ff99'])\n",
        "    ax.set_ylabel('Value', fontsize=12)\n",
        "    ax.set_title('Model Efficiency Metrics', fontsize=14, fontweight='bold')\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, val in zip(bars, values):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                f'{val:.2f}', ha='center', va='bottom', fontsize=11)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Results Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"FINAL RESULTS SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nDataset:\")\n",
        "print(f\"  - Training samples: {len(train_dataset_full):,}\")\n",
        "print(f\"  - Validation samples: {len(val_dataset_full):,}\")\n",
        "print(f\"  - Test samples: {len(test_dataset_full):,}\")\n",
        "\n",
        "print(f\"\\nModel:\")\n",
        "print(f\"  - Architecture: CNN (3 Conv + 2 FC layers)\")\n",
        "print(f\"  - Parameters: {trained_model.count_parameters():,}\")\n",
        "print(f\"  - Size: {trained_model.get_model_size_mb():.2f} MB\")\n",
        "\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  - Test Accuracy: {results['accuracy']*100:.2f}%\")\n",
        "print(f\"  - Test Precision: {results['precision']*100:.2f}%\")\n",
        "print(f\"  - Test Recall: {results['recall']*100:.2f}%\")\n",
        "print(f\"  - Test F1-Score: {results['f1']*100:.2f}%\")\n",
        "\n",
        "if 'inference_time' in results:\n",
        "    print(f\"\\nEfficiency:\")\n",
        "    print(f\"  - Inference time: {results['inference_time']['avg_sample_time_ms']:.2f} ms/sample\")\n",
        "    print(f\"  - Throughput: {results['inference_time']['samples_per_second']:.2f} samples/sec\")\n",
        "\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Conclusion\n",
        "\n",
        "### What Worked\n",
        "- CNN architecture successfully learned patterns from QR code images\n",
        "- Data augmentation improved model generalization\n",
        "- Gradient clipping prevented training instability\n",
        "- Wandb integration provided excellent visualization of training progress\n",
        "\n",
        "### What Didn't Work Initially\n",
        "- Initial learning rate (0.001) was too high, causing poor convergence\n",
        "- Model was too large initially, making training difficult\n",
        "- Insufficient data (5K per class) limited model learning\n",
        "- Weight decay (0.01) was too aggressive\n",
        "\n",
        "### Solutions Applied\n",
        "1. **Reduced learning rate**: 0.001 → 0.0005 → 0.0001\n",
        "2. **Increased dataset size**: 5K → 20K per class\n",
        "3. **Added gradient clipping**: Prevents exploding gradients\n",
        "4. **Reduced weight decay**: 0.01 → 0.001\n",
        "5. **Better initialization**: Small init for final layer\n",
        "6. **More epochs**: 10 → 30 (then back to 10 for time)\n",
        "\n",
        "### Future Improvements\n",
        "1. **Transfer Learning**: Use pretrained models (ResNet, EfficientNet)\n",
        "2. **Hybrid Approach**: Combine image features with URL features from CSV files\n",
        "3. **Ensemble Methods**: Combine multiple models for better accuracy\n",
        "4. **Hyperparameter Tuning**: Use WandB Sweeps for automated tuning\n",
        "5. **More Data**: Use full dataset (1M images) instead of sampling\n",
        "6. **Different Architecture**: Try Vision Transformers or attention mechanisms\n",
        "7. **Data Quality**: Investigate if QR codes have visual differences or if URL analysis is needed\n",
        "\n",
        "### Key Learnings\n",
        "- QR codes are visually very similar, making classification challenging\n",
        "- Data size and quality are crucial for model performance\n",
        "- Proper hyperparameter tuning is essential for convergence\n",
        "- Monitoring training with validation set prevents overfitting\n",
        "- Gradient clipping helps stabilize training\n",
        "- Lower learning rates often work better for complex tasks\n",
        "\n",
        "### Final Notes\n",
        "This project demonstrates a complete deep learning pipeline for binary image classification. The model can be further improved with more data, better architectures, or hybrid approaches combining visual and URL features.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
